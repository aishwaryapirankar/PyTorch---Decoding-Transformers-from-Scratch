# PyTorch---Decoding-Transformers-from-Scratch

This repository contains a PyTorch implementation of a Transformer decoder, built from scratch and organized using PyTorch and Lightning.

## Overview

This project provides a clear and educational implementation of a Transformer decoder, focusing on the core components and their interactions. It aims to demystify the inner workings of Transformer-based models, particularly the decoding process. The code is structured using PyTorch and Lightning to enhance readability, reproducibility, and ease of experimentation.

## Contents

-   **`decoder-from-scratch-pytorch-lightning.ipynb`**: A Jupyter Notebook demonstrating the complete implementation of the Transformer decoder using PyTorch Lightning. This notebook includes:
    -      Implementation of positional encodings.
    -      Step-by-step construction of the decoder layers (masked multi-head attention, feed-forward network).
    -      Integration with PyTorch Lightning for training and evaluation.
    -      Example usage and demonstration of the decoder's functionality.

## Features

-      **From Scratch Implementation**: Provides a detailed, step-by-step implementation of the Transformer decoder.
-   **PyTorch Lightning**: Leverages PyTorch Lightning for clean, organized, and scalable code.
-   **Educational Focus**: Designed to be easily understandable, making it ideal for learning and experimentation.
-   **Masked Multi-Head Attention**: Implements the crucial masked multi-head attention mechanism for autoregressive decoding.
-   **Positional Encodings**: Includes implementation of positional encodings to capture sequence order.

## Requirements

-      Python 3.x
-      PyTorch
-      PyTorch Lightning

## Acknowledgements

This project was inspired and influenced by the clear and intuitive explanations of machine learning concepts provided by StatQuest with Joshua Starmer. Their educational videos and content have been invaluable in understanding the fundamentals of machine learning and neural networks, which are essential for building and understanding Transformer models. (https://github.com/StatQuest/decoder_transformer_from_scratch/blob/main/decoder_transformers_with_pytorch_and_lightning_v2.ipynb)

